%--------------------------------------------------------------------------------------
% Este arquivo contém a sua metodologia
%--------------------------------------------------------------------------------------
\chapter{Materiais e Métodos} \label{sec:newModule} %Uma label é como você referencia uma seção no texto com a tag \ref{}

Como já especificado, o trabalho tem como objetivo a investigação de modelos de
mundo a serem utilizados no desenvolvimento de um módulo de defesa para o time de
Futebol de Robôs na categoria de simulação 2D, FutVasf2D. O processo de
modelagem de um mundo em um MDP exige a discretização do mesmo de forma que
sejam obtidos um número finito de ações e de estados com os quais o algoritmo de
aprendizado de reforço possa trabalhar, sendo assim, a próxima etapa é definir
como o algoritmo escrito pode ser implementado no time base escolhido
(\textit{WrightEagleBASE}).

\section{Modelagem do Mundo}\label{modelagem}

Os trabalhos apresentados na Seção \ref{trabalhosRelacionados} oferecem várias
possibilidades de como adaptar o ambiente da partida para um problema que se
encaixe às definições de MDP. Algumas das variáveis destacadas por
\citeonline{gabel2008case}, \citeonline{celiberto2007heuristic},
\citeonline{bianchi2010case} e \citeonline{homem2017improving} estão descritos a
seguir:

\begin{itemize}
    \item Posição do jogador;
    \item Posição da bola;
    \item Orientação do corpo do jogador;
    \item Distância entre o jogador e a bola;
    \item Velocidade do jogador;
    \item Distância entre jogador e adversário mais próximo;
    \item Ângulos diversos, como ângulo entre jogador e adversário, ângulo
    formado entre bola, jogador e gol adversário.
\end{itemize}

Partindo das informações e exemplos obtidos através da revisão bibliográfica,
foram modeladas algumas variações de de formas de representar o mundo. Quatro
componentes são importantes neste processo, os estados que podem ser alcançados
pelo agente, as ações que o mesmo pode tomar, as recompensas e punições a serem
atribuídas para o resultado das ações em cada estado e como será feita a
implementação de todo o modelo de mundo no agente. Os 3 primeiros componentes são
cobertos nas subseções abaixo enquanto a implementação é abordada na Seção
\ref{implementacao}.

\subsection{Estados}\label{states}

A definição dos estados é uma etapa certamente crucial e complexa.
É necessário um cuidado sobre a questão de quais serão os possíveis valores
de cada variável, visto que esta decisão impactará na complexidade e
quantidade de estados além do significado de cada um.

Várias das variáveis abordadas a seguir possem
caráter contínuo e uma amplitude de possíveis valores muito grande, por conta
disto foi feita a discretização do domínio em 3 classes através da análise
dos valores assumidos pelas variáveis na execução de várias partidas.

Neste trabalho foram tomados dois conjuntos de estados que serão tratados nas
subseções seguintes.

\subsubsection{Conjunto 1}

Este conjunto define 7 variáveis escolhidas a partir da revisão dos estados
adotados pelos trabalhos visitados no Capítulo \ref{sec:fundamentation} e estudo
do problema proposto. As variáveis e seus respectivos domínios são:

\begin{itemize}
    \item \textbf{Posição do jogador em relação à bola no eixo horizontal}
    ($P_{hj}$), ou seja, se o jogador está mais a direita ou a esquerda da
    bola, esta é uma variável booleana onde VERDADEIRO implica
    que o jogador está a esquerda e FALSO a direita;
    \item \textbf{Posição do jogador em relação à bola no eixo vertical}
    ($P_{vj}$), se identifica se o jogador pode ser encontrado mais acima ou
    abaixo da bola, assim como o item passado, trata-se de uma variável
    booleana, onde VERDADEIRO significa que o jogador está acima da bola e FALSO, abaixo,
    esta variável e a anterior são úteis para questão da direção de movimentação
    do jogador;
    \item \textbf{Distância entre o jogador e bola} ($D_{jb}$), mede o
    afastamento do agente em relação à bola, o domínio das
    variáveis de distância é o mesmo (PERTO, MÉDIO e LONGE), diferindo
    apenas nos valores atribuídos a cada classe, nesta, os valores para PERTO é
    sempre menor ou igual a 10 (unidade de medida utilizada pelo servidor), para
    MÉDIO os valores são entre 10 e 20 e LONGE engloba todos acima de 20;
    \item \textbf{Distância entre o aliado mais próximo da bola e a bola}
    ($D_{tb}$), aqui os intervalos são menores, podendo assumir PERTO quando o
    valor da distância é menor ou igual a 5, MÉDIO quando entre 5 e 15 e LONGE
    quando acima de 15;
    \item \textbf{O agente é o aliado mais próximo da bola} ($F$), trata-se de
    uma variável booleana que identifica se o jogador ``pensante'' é o aliado
    mais próximo à bola;
    \item \textbf{Distância do adversário com posse de bola do gol} ($D_{ag}$),
    mede o afastamento do jogador com posse de bola do gol aliado, aqui os
    valores para PERTO são todos os que forem iguais ou abaixo de 11, MÉDIO
    quando entre 11 e 22 e LONGE quando acima de 33;
    \item \textbf{Compactação} ($\sigma$), consiste numa variável construída a
    partir das distâncias dos jogadores entre si, com foco no adversário com
    posse de bola. Esta variável é importante pois serve como uma maneira de
    reconhecer a dispersão dos jogadores no campo possibilitando o fechamento de
    um ``cerco'' com menos brechas em torno da bola. O cálculo desta variável é
    feito como um desvio padrão com base na distância média entre os jogadores
    aliados e o jogador com posse de bola, como na Equação \ref{eq:desvio}, onde
    $d_i$ é a distância entre o jogador $i$ e o adversário com posse de bola e
    $m$ é a média dessas distâncias, o domínio desta variável é dado por
    ESPARSO (valores acima de 14), COMPACTO (entre 14 e 7) e MUITO COMPACTO
    (abaixo de 7).
\end{itemize}

\begin{equation}
    \sigma=\sqrt{0,1\times \sum{(d_i - m)^2}}
    \label{eq:desvio}
\end{equation}

O estado do modelo de mundo descrito, então, passa a ser representado por uma 7-tupla,
($P_{hj}$, $P_{vj}$, $D_{jb}$, $D_{tb}$, $F$, $D_{ag}$, $\sigma$). Como
consequência da quantidade de variáveis e de seus valores, o campo de estados
passa a assumir 648 possibilidades, de acordo com a Equação
\ref{eq:stateNumber}, onde $N$ é o número de estados, $n$ a quantidade de
variáveis e $V_i$ é tamanho do domínio da variável $i$.

\begin{equation}\label{eq:stateNumber}
    N=\prod\limits_{i=0}^{n}{V_i}
\end{equation}

Os 648 estados foram montados seguindo a mesma lógica aplicada à formação de uma
tabela verdade, uma pequena amostra desta montagem está representada pela Tabela
\ref{tab:states1}.

\begin{table}[hbt]
    \centering
    \begin{tabular}{c|c|c|c|c|c|c}
        $P_{hj}$    &   $P_{vj}$    &   $D_{jb}$    &   $D_{tb}$    &   $F$         &   $D_{ag}$    &   $\sigma$ \\ \hline
        V  &   V  &   P       &   P       &   V  &   P       &   E \\
        V  &   V  &   P       &   P       &   V  &   P       &   C \\
        V  &   V  &   P       &   P       &   V  &   P       &   MC \\
        V  &   V  &   P       &   P       &   V  &   M       &   E \\
        V  &   V  &   P       &   P       &   V  &   M       &   C \\
        V  &   V  &   P       &   P       &   V  &   M       &   MC \\
        V  &   V  &   P       &   P       &   V  &   M       &   E \\
        V  &   V  &   P       &   P       &   V  &   L       &   C \\
        V  &   V  &   P       &   P       &   V  &   L       &   MC \\
        V  &   V  &   P       &   P       &   F       &   L       &   E \\
        V  &   V  &   P       &   P       &   F       &   P       &   C \\
        V  &   V  &   P       &   P       &   F       &   P       &   MC \\
        \dots       &   \dots       &   \dots       &   \dots       &   \dots
        &   \dots       &   \dots \\
        F       &   F       &   L       &   L       &   V  &   L       &   E \\
        F       &   F       &   L       &   L       &   V  &   L       &   C \\
        F       &   F       &   L       &   L       &   V  &   L       &   MC \\
        F       &   F       &   L       &   L       &   F       &   P       &   E \\
        F       &   F       &   L       &   L       &   F       &   P       &   C \\
        F       &   F       &   L       &   L       &   F       &   P       &   MC \\
        F       &   F       &   L       &   L       &   F       &   M       &   E \\
        F       &   F       &   L       &   L       &   F       &   M       &   C \\
        F       &   F       &   L       &   L       &   F       &   M       &   MC \\
        F       &   F       &   L       &   L       &   F       &   L       &   E \\
        F       &   F       &   L       &   L       &   F       &   L       &   C \\
        F       &   F       &   L       &   L       &   F       &   L       &   MC \\
    \end{tabular}
    \caption{\textit{Montagem de Conjunto de Estados 1 (V = VERDADEIRO, F = FALSO, P = PERTO, M = MÉDIO, L = LONGE, E = ESPARSO, C = COMPACTO e MC = MUITO COMPACTO)}}
    \label{tab:states1}
\end{table}

\subsubsection{Conjunto 2}

O segundo conjunto de estados possui apenas uma váriavel que se baseia em
conceitos básicos de defesa no futebol real.

Foram identificados os papéis de Primeira Defesa, Segunda Defesa e Terceira
defesa para o jogador mais próximo da bola, o segundo mais próximo e o terceiro
mais próximo, respectivamente. Além disso, o valor de compactação já descrito
foi reaproveitado para implementar o conceito de balanceamento no restante do
time, resultando na modificação da Equação \ref{eq:desvio} para a Equação
\ref{eq:desvio2} e utilizando o mesmo conceito de classe anterior, atribuindo-o
como possível valor para quando o jogador não assume nenhum dos papéis anteriores.

\begin{equation}
    \sigma=\sqrt{\frac{\sum{(d_i - m)^2}}{7}}
    \label{eq:desvio2}
\end{equation}

Desta forma essa abordagem resulta num campo de 7 estados que são:

\begin{itemize}
    \item \textbf{Primeira Defesa}, se o agente pensante assume o papel de
    primeira defesa;
    \item \textbf{Primeira Defesa Muito Perto}, se o agente pensante assume o papel de
    primeira defesa e está a uma distância menor oi igual a 5 da bola;
    \item  \textbf{Segunda Defesa}, se o agente tomando a decisão assume o papel
    de segunda defesa;
    \item \textbf{Terceira Defesa}, se agora é assumido o papel de terceira
    defesa;
    \item \textbf{Esparso}, se o jogador não se encaixa nos estados anteriores e
    os jogadores estão muito espalhados no campo;
    \item \textbf{Compacto}, se os jogadores estão menos espalhados;
    \item \textbf{Muito Compacto}, se os jogadores sem papel definido estão
    juntos no campo;
\end{itemize}

Esses foram os estados utilizados nos experimentos descritos no decorrer deste
trabalho, na subseção seguinte será abordada a questão da escolha de possíveis
ações para os agentes.

\subsection{Ações}\label{actions}

A escolha das ações foi pensando no que pode ser feito, enquanto no papel de
defesa numa partida de futebol, a fim de capturar a bola e impedir o avanço do
time adversário. A implementação original do time base define 3 possíveis ações
para a situação de defesa, mover-se para uma melhor posição, bloquear o avanço
do jogador com posse de bola e marcar adversários para impedir a recepção de
bola. Para este trabalho foram mantidas estas ações com algumas modificações e
adições.

Primeiramente, a ação de se mover foi divida em 5 possíveis ações, mover-se para
cima, para baixo, para esquerda, para direita ou diretamente em direção a bola, desta forma é eliminada a
avaliação original do time base garantido que o aprendizado seja exclusivamente
baseado no algoritmo \textit{Q-Learning}.

Além dessa divisão foi adicionada a ação de interceptação e a possibilidade de
não fazer nada. A ação de interceptação consiste na tentativa de dominar a bola
quando a mesma está ao alcance do agente.


Desta forma, o domínio das ações passa a ser considerado:

\begin{itemize}
    \item Mover-se para cima;
    \item Mover-se para baixo;
    \item Mover-se para direita;
    \item Mover-se para esquerda;
    \item Mover-se em direção à bola;
    \item Bloquear avanço do adversário com posse de bola;
    \item Marcar adversários;
    \item Interceptar bola;
    \item Não fazer nada.
\end{itemize}

\subsection{Recompensas}\label{rewards}

As recompensas são partes essenciais do aprendizado por reforço, visto que são
elas as responsáveis por informar ao agente se a ação tomada obteve resultados
positivos ou negativos e o quão positivo ou negativo foi a ação. Esta decisão
não é fácil de ser tomada e por isso foram feitas algumas implementações a fim
de procurar otimizar esses valores.

A primeira tentativa de modelar um bom sistema de recompensas se baseou na
ocorrência dos principais eventos na simulação, gols, bolas jogadas para fora do
campo, captura de bola e passes. Esta modelagem adotou como recompensa para
captura de bola +20, para gols sofridos -20 e para bolas lançadas para fora o
valor +10, visto que implica no sucesso na tentativa de impedir possíveis ações para o
agente adversário, convenção mantida em todas as
modelagens seguintes, e para passe foi atribuído o valor -5 numa tentativa de
impedir o time adversário de se articular. A Tabela \ref{tab:rewards1} mostra a
relação de consequências e recompensas para esta primeira implementação.

\begin{table}[hbt]
    \centering
    \begin{tabular}{c|c}
        Consequência    &   Recompensa  \\ \hline
        Captura da bola &   +20 \\
        Gol             &   -20 \\
        Fora            &   +10 \\
        Passe           &   -5  \\
    \end{tabular}
    \caption{\textit{Consequência X Recompensa - 1}}
    \label{tab:rewards1}
\end{table}

A segunda implementação aboliu a recompensa de passes e adicionou uma
verificação no avanço e recuo da bola, atribuindo -5 ao avanço da bola, +5 ao
recuo da bola somada à aproximação do agente da captura da bola e +2 ao recuo da
bola sem avanço dos aliados. Esta modelagem mostrou a presença de 
punição excessiva no avanço da bola, visto que tal avanço é uma
característica constante nas situações simuladas, desta forma a última
implementação removeu esta punição e atribuiu recompensa neutra a esta e como
padrão para não identificação de nenhuma das situações anteriores. A relação
consequência recompensa desta implementação está exposta na Tabela
\ref{tab:rewards2}.

\begin{table}[hbt]
    \centering
    \begin{tabular}{c|c}
        Consequência    &   Recompensa  \\ \hline
        Captura da bola &   +20 \\
        Gol             &   -20 \\
        Fora            &   +10 \\
        Recuo da bola   &   +2  \\
        Recuo da bola e
        aproximação do
        agente          &   +5 \\
    \end{tabular}
    \caption{\textit{Consequência X Recompensa - 2}}
    \label{tab:rewards2}
\end{table}

\section{Implementação em \textit{WrightEagleBASE}}\label{implementacao}

O último ponto a ser abordado sobre a modelagem está na implementação do
algoritmo, mas para que se possa tratar da implementação realizada, se faz
necessário primeiro entender o funcionamento atual do time base escolhido.

O time base \textit{WrightEagleBASE} conta com um mecanismo modularizado para
tomada de decisões. A sua implementação depende do ``tipo'' de classe
\textit{Behavior} (comportamento), que é constituído por classes mais
especializadas, como comportamento de bloqueio ou de interceptação, e menos
especializadas, como comportamento de defesa ou ataque. As classes menos
especializadas, na implementação original, criam uma forma de hierarquia para
execução das mais especializadas que irão decidir se deve tomar a ação
correspondente a classe ou não. 

Este trabalho foca em 5 dessas classes, \textit{BehaviorDefensePlanner},
\textit{BehaviorFomationPlanner}, \textit{BehaviorBlockPlanner},
\textit{BehaviorMarkPlanner} e \textit{BehaviorInterceptPlanner}, que são
descritas a seguir. O diagrama representado na Figura \ref{img:classDiagram},
mostra como é o diagrama de classes simplificado da implementação original
focando nas classes \textit{Behavior}, mas especificamente na
\textit{BehaviorDefensePlanner}, vale destacar que a classe
\textit{BehaviorPlannerBase} corresponde a uma classe abstrata que é
implementada por todas as demais.

\begin{itemize}
    \item \textbf{\textit{BehaviorDefensePlanner}} decide entre quais
    comportamentos o agente deve decidir quando o time está sem posse de bola e
    em qual ordem, na implementação original apenas define uma ordem de execução
    dos testes de cada possível ação de defesa;
    \item \textbf{\textit{BehaviorFormationPlanner}} coordena a movimentação do
    time, decide quando o agente precisa ser realocado;
    \item \textbf{\textit{BehaviorBlockPlanner}} é responsável por definir
    quando o agente deve tentar bloquear o adversário;
    \item \textbf{\textit{BehaviorMarkPlanner}} decide se há a necessidade de
    determinado agente marcar um adversário;
    \item \textbf{\textit{BehaviorInterceptPlanner}} busca a possibilidade de
    interceptar a bola e se deve tentar realizar tal ação.
\end{itemize}

\imagem{0.5}{classDiagram}{Diagrama de classes simplificado para mecanismo de defesa.}{O Autor}

Um exemplo de como isso é aplicado é dado pela implementação original do
mecanismo de defesa que acontece da seguinte maneira. A classe de defesa define
que a ordem de execução dos planejadores mais específicos é a seguinte:

\begin{enumerate}
    \item \textit{BehaviorFormationPlanner}
    \item \textit{BehaviorBlockPlanner}
    \item \textit{BehaviorMarkPlanner}
\end{enumerate}

Desta forma, o agente primeiro decidirá se precisa se movimentar e para que
local e só em seguida decidirá se precisa realizar um bloqueio e apenas depois
disso, se precisa marcar algum jogador e qual. Desta forma, se garante que o
jogador só se preocupará com bloqueio ou marcação depois de estar na posição
adequada. O fluxograma do algoritmo de decisão das 5 classes,
\textit{BehaviorDefensePlanner}, \textit{BehaviorFormationPlanner},
\textit{BehaviorBlockPlanner}, \textit{BehaviorMarkPlanner}
e \textit{BehaviorInterceptPlanner} podem ser vistas nas
Figuras \ref{img:BehaviorDefense}, \ref{img:BehaviorFormation},
\ref{img:BehaviorBlock}, \ref{img:BehaviorMark} e \ref{img:BehaviorIntercept}, respectivamente.

\imagem{0.8}{BehaviorDefense}{Algoritmo da classe \textit{BehaviorDefensePlanner}}{O Autor}
\imagem{0.5}{BehaviorFormation}{Algoritmo da classe \textit{BehaviorFormationPlanner}}{O Autor}
\imagem{0.8}{BehaviorBlock}{Algoritmo da classe \textit{BehaviorBlockPlanner}}{O Autor}
\imagem{0.8}{BehaviorMark}{Algoritmo da classe \textit{BehaviorMarkPlanner}}{O Autor}
\imagem{0.5}{BehaviorIntercept}{Algoritmo da classe \textit{BehaviorInterceptPlanner}}{O Autor}

Na implementação dos modelos, as classes mais especializadas
(\textit{BehaviorMarkPlanner, BehaviorBlockPlanner, BehaviorFormationPlanner} e
\textit{BehaviorInterceptPlanner}) foram modificadas ou deixaram de ser
utilizadas para melhor se adequar ao processo de tomada de ação proposto.

Desde a primeira implementação do mecanismo de movimento a classe
\textit{BehaviorFormationPlanner} deixou de ser utilizada para dar lugar a
implementação direta do mecanismo de movimentação provido pelo
\textit{framework} do time base. Em uma implementação futura o mesmo foi feito
com as demais classes, mas a principio as classes foram apenas modificadas
removendo as condições que decidiam se a ação deveria ou não ser executada.

Além das classes \textit{Behavior}, são importantes citar duas classes
modificadas no desenvolvimento deste trabalho, são elas \textit{Player} e
\textit{Agent}. A primeira foi utilizada para a implementação da utilização da
\textit{Q-Table} por se tratar de uma etapa esterna aos comportamentos
possibilitando a detecção de eventos que encerra o episódio, como a captura de
defesa, que fazem com que a classe \textit{BehaviorDefensePlanner} não seja
acessada no instante devido. Já a classe \textit{Agent} foi utilizado apenas
como meio de guardar informações sobre o agente a ser compartilhados entre as
classes, visto que esta é a classe responsável pela descrição do agente.

A classe \textit{Player}, portanto, foi tomada com uma certa preocupação devido
ao tempo (medido em ciclos) para se considerar que a ação tomada obteve
resultados. Para tratar desta questão, foi desenvolvido uma fila de ações e seus
respectivos estados para que depois de um determinado número de ciclos pudessem
ser avaliadas, com exceção de estados terminais (como gols, capturas e gols fora
do campo) que quando ocorrem é tomada a primeira ação da fila e em seguida
limpada novamente, tomando assim a ação mais velha que poderia ter causado o
alcance a tal estado. O tamanho da fila foi alterado para tentar chegar ao
melhor valor, assumindo os valores de 2 ciclos, 5 ciclos, 10 ciclos ou 1 ciclo
(ação imediata).

Vale destacar ainda a implementação da \textit{Q-Table} em si, a principio houve
a tentativa de utilizar apenas um arquivo binário com a estrutura de dados de
matriz de \textit{double} para guardar as informações entre todos os agentes
aliados  em campo, porém esta abordagem se mostrou defeituosa por conta da
concorrência do acesso.

A questão é que o time base não faz utilização de \textit{threads}, utilizando
clientes completamente idependentes entre si, em relação a processos, o que
fazia da única forma de controlar o acesso ao arquivo a criação de exclusão de
um arquivo de trava. O problema surgido desta abordagem é a velocidade de
execução dos ciclos que acarretava na verificação por parte de mais de um agente
no momento em que o arquivo de trava ainda não havia sido criado por nenhum
outro, causando sobreposição da tabela sempre que um novo agente terminava de
processar uma ação.

A solução deste problema foi a criação de uma tabela para cada jogador, isto
resolve  o problema da concorrência e trata cada jogador por sua função
especifica no jogo, mas torna o aprendizado potencialmente mais
lento, devido a falta de compartilhamento de experiência. Outra questão desta
abordagem é que obriga a execução do treinamento de todos os jogadores ao mesmo tempo.

\section{Metodologia Experimental}\label{meotodologia}

Os modelos de mundo propostos foram treinados com o auxilio da subtarefa HFO,
mostrado na Figura \ref{img:hfo}, que consiste numa especialização do
\textit{RoboCup Simulated Soccer} onde é utilizado apenas metade do campo com
alguns jogadores no ataque e alguns na defesa, foram experimantadas seções de
treinamento utilizando 10 contra 10 jogadores. Com o HFO, é
possível treinar os jogadores para defesa de forma mais objetiva focando apenas
em situações de defesa. Seria possível a execução do treinamento utilizando
partidas completas, no entanto, isso tornaria o treinamento mais lento, já que
seriam encontradas muitas situações diferentes do alvo do trabalho,
resultando em muito tempo na execução de uma partida para poucos episódios
treinados.

\imagem{0.32}{hfo}{HFO}{\url{http://www.cs.utexas.edu/~AustinVilla/sim/halffieldoffense/}}

Devida necessidade de comparar vários modelos de mundo e o quão cada um evolue com o
tempo foi utilizado o mecanismo de treinamento para também verificar o
desempenho do modelo em execução. Para este fim foram feitas modificações no
código-fonte com o intuito de contabilizar o número de gols sofridos em
intervalos de 3000 ciclos (metade da quantidade de ciclos de uma partida comum
da categoria) e foram registradas as variações destes resultados para cada
modelo, inclusive o time original sem a técnica empregada.