<img 
src="main0x.png" alt="PICT" >
                                                                                       

       <span 
class="ecrm-1200">figure.1</span>
       <span 
class="ecrm-1200">figure.1</span>
       <span 
class="ecrm-1200">figure.1</span>
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"  
  "http://www.w3.org/TR/html4/loose.dtd">  
<html xml:lang="pt" > 
<head><title></title> 
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"> 
<meta name="generator" content="TeX4ht (http://www.tug.org/tex4ht/)"> 
<meta name="originator" content="TeX4ht (http://www.tug.org/tex4ht/)"> 
<!-- html,htex4ht --> 
<meta name="src" content="main.tex"> 
<link rel="stylesheet" type="text/css" href="main.css"> 
</head><body 
>
<!--l. 8--><p class="noindent" ><hr class="figure"><div class="figure" 
>
                                                                                       <img 
src="main1x.png" alt="PICT" >
<img 
src="main2x.png" alt="PICT" >
                                                                                       
                                                                                       <img 
src="main3x.png" alt="PICT" >
<img 
src="main4x.png" alt="PICT" >
                                                                                       
<div class="center" 
>
<!--l. 9--><p class="noindent" >
<!--l. 10--><p class="noindent" > <img 
src="img/LOGO_UNIVASF_big-.png" alt="PIC"  
></div>
                                                                                       <img 
src="main5x.png" alt="PICT" >
<img 
src="main6x.png" alt="PICT" >
                                                                                       
<!--l. 12--><p class="noindent" ></div><hr class="endfigure">
<!--l. 16--><p class="noindent" ><span 
class="ec-lmb-10x-x-144">UNIVERSIDADE FEDERAL DO VALE DO S</span><span 
class="ec-lmb-10x-x-144">ÃO FRANCISCO</span>
<!--l. 16--><p class="noindent" ><span 
class="ec-lmb-10x-x-144">CURSO DE GRADUA</span><span 
class="ec-lmb-10x-x-144">Ç</span><span 
class="ec-lmb-10x-x-144">ÃO EM ENGENHARIA DE COMPUTA</span><span 
class="ec-lmb-10x-x-144">Ç</span><span 
class="ec-lmb-10x-x-144">ÃO</span>
<!--l. 19--><p class="noindent" >JOÃO PEDRO FIGUEIRÔA NASCIMENTO
<div class="center" 
>
<!--l. 22--><p class="noindent" >
<!--l. 23--><p class="noindent" ><span 
class="ec-lmbx-12x-x-120">INVESTIGA</span><span 
class="ec-lmbx-12x-x-120">Ç</span><span 
class="ec-lmbx-12x-x-120">ÃO SOBRE UTILIZA</span><span 
class="ec-lmbx-12x-x-120">Ç</span><span 
class="ec-lmbx-12x-x-120">ÃO DE</span>
<span 
class="ec-lmbx-12x-x-120">APRENDIZAGEM POR REFOR</span><span 
class="ec-lmbx-12x-x-120">ÇO PARA M</span><span 
class="ec-lmbx-12x-x-120">ÓDULO DE</span>
<span 
class="ec-lmbx-12x-x-120">DEFESA APLICADO AO FUTVASF2D</span></div>
<!--l. 27--><p class="noindent" ><span 
class="ec-lmbx-12x-x-120">JUAZEIRO - BA</span><br />
<span 
class="ec-lmbx-12x-x-120">2019</span>
                                                                                       <img 
src="main7x.png" alt="PICT" >
<img 
src="main8x.png" alt="PICT" >
                                                                                       
                                                                                       <img 
src="main9x.png" alt="PICT" >
<img 
src="main10x.png" alt="PICT" >
                                                                                       
                                                                                       <img 
src="main11x.png" alt="PICT" >
<img 
src="main12x.png" alt="PICT" >
                                                                                       
<!--l. 36--><p class="indent" >       <a 
 id="likesection.1"></a><a 
 id="chapterb1.0"></a>
<!--l. 37--><p class="noindent" ><span 
class="ec-lmb-10x-x-144">UNIVERSIDADE FEDERAL DO VALE DO S</span><span 
class="ec-lmb-10x-x-144">ÃO FRANCISCO</span>
<!--l. 37--><p class="noindent" ><span 
class="ec-lmb-10x-x-144">CURSO DE GRADUA</span><span 
class="ec-lmb-10x-x-144">Ç</span><span 
class="ec-lmb-10x-x-144">ÃO EM ENGENHARIA DE COMPUTA</span><span 
class="ec-lmb-10x-x-144">Ç</span><span 
class="ec-lmb-10x-x-144">ÃO</span>
<!--l. 40--><p class="noindent" >JOÃO PEDRO FIGUEIRÔA NASCIMENTO
<!--l. 44--><p class="noindent" ><span 
class="ec-lmbx-12x-x-120">INVESTIGA</span><span 
class="ec-lmbx-12x-x-120">Ç</span><span 
class="ec-lmbx-12x-x-120">ÃO SOBRE UTILIZA</span><span 
class="ec-lmbx-12x-x-120">Ç</span><span 
class="ec-lmbx-12x-x-120">ÃO DE</span>
<span 
class="ec-lmbx-12x-x-120">APRENDIZAGEM POR REFOR</span><span 
class="ec-lmbx-12x-x-120">ÇO PARA M</span><span 
class="ec-lmbx-12x-x-120">ÓDULO DE</span>
<span 
class="ec-lmbx-12x-x-120">DEFESA APLICADO AO FUTVASF2D</span>
<!--l. 47--><p class="noindent" >&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;
<div class="minipage">Trabalho apresentado à Universidade Federal
do Vale do São Francisco - Univasf, Campus
Juazeiro,  como  requisito  da  obtenção  do
título   de   Bacharel   em   Engenharia   de
Computação.<br 
class="newline" /><br 
class="newline" />
<!--l. 52--><p class="noindent" >Orientador:&#x00A0;Prof.  Dr.  Rosalvo  Ferreira  de
Oliveira
<!--l. 53--><p class="noindent" >&#x00A0;
</div> <span 
class="ec-lmbx-12x-x-120">JUAZEIRO - BA</span><br />
<span 
class="ec-lmbx-12x-x-120">2019</span>
                                                                                       <img 
src="main13x.png" alt="PICT" >
<img 
src="main14x.png" alt="PICT" >
                                                                                       
<!--l. 164--><p class="noindent" >
                                                                                       <img 
src="main15x.png" alt="PICT" >
<img 
src="main16x.png" alt="PICT" >
                                                                                       
<!--l. 166--><p class="indent" >       <a 
 id="likesection.2"></a><a 
 id="chapterb2.0"></a>
                                                                                       <div class="flushright" 
>
<!--l. 167--><p class="noindent" >
 <span 
class="ec-lmri-12">Planet Earth is blue and there&#8217;s nothing I can do</span><br />
<span 
class="ec-lmbx-12">David Bowie</span><br />
<span 
class="ec-lmbx-12">Space Oddity</span></div>
                                                                                       <img 
src="main17x.png" alt="PICT" >
<img 
src="main18x.png" alt="PICT" >
                                                                                       
<!--l. 177--><p class="noindent" ><a 
 id="likesection.3"></a><a 
 id="chapterb4.0"></a>
                                                                                       <img 
src="main19x.png" alt="PICT" >
<img 
src="main20x.png" alt="PICT" >
                                                                                       
       <h2 class="likechapterHead"><a 
 id="x1-1000"></a>Agradecimentos</h2>
<!--l. 178--><p class="noindent" >Agradeço primeiramente a Deus e a minha família, principalmente meus pais, João Alves e
Regina Coeli, e a meus irmãos, Maria Alice e Luciano, pelo apoio e carinho durante todo o
caminho da graduação.
<!--l. 182--><p class="indent" >       À minha noiva, Nathanaele, por mesmo longe se fazer presente e me incentivar a
continuar em frente sempre.
<!--l. 185--><p class="indent" >       Agradeço ao &#8220;Clubinho Guaraná&#8221; composto por Ellen, Daniel, Talita, Carolina, Isaac,
Esron e Mauricio, pelos momentos compartilhados de alegria, estresse e, as vezes, tristeza. Um
grupo não só de leitura, mas de colaboração mútua e respeito. Um agradecimento especial a
Ellen por estar comigo desde o começo da jornada.
<!--l. 191--><p class="indent" >       Agradeço a meu orientador, Rosalvo, por me apresentar a oportunidade de dar inicio
ao projeto FutVasf2D e trabalhando comigo por 2 anos seguidos até finalmente a
conclusão.
<!--l. 195--><p class="indent" >       Obrigado aos professores Ricardo Ramos e Jorge Cavalcanti que contribuíram com a
melhoria deste trabalho.
<!--l. 198--><p class="indent" >       Obrigado ainda a Brauliro e Ricardo, novamente, com conselhos profissionais e
acadêmicos que contribuíram para formação e ofereceram momentos de lazer que se
mostraram um óasis em meio a todo o estresse do curso, se tornando amigos, ouso
dizer.
<!--l. 202--><p class="indent" >       Obrigado à Deise por sempre ajudar com toda a burocracia da UNIVASF, até quando
parecia que não tinha jeito.
<!--l. 205--><p class="indent" >       Por fim agradeço a todos que estiveram comigo neste caminho, professores, colegas e
amigos. Obrigado por me ajudar a me tornar quem eu sou hoje.
                                                                                       <img 
src="main21x.png" alt="PICT" >
<img 
src="main22x.png" alt="PICT" >
                                                                                       
<!--l. 231--><p class="indent" >       <a 
 id="likesection.4"></a><a 
 id="chapterb5.0"></a>
<div class="center" 
>
<!--l. 231--><p class="noindent" >
<!--l. 231--><p class="noindent" ><span 
class="ec-lmb-10x-x-120">RESUMO</span></div>
<!--l. 235--><p class="noindent" >Este trabalho se debruça sobre a copa do mundo de futebol de robôs (<span 
class="ec-lmri-12">RoboCup Soccer</span>),
que  incentiva  a  produção  de  pesquisas  na  área  de  inteligência  artificial  e  robótica,
enquanto parte do projeto FutVasf2D que busca o desenvolvimento de um time oficial
de futebol de robôs na liga de simulação 2D para UNIVASF, investigando o efeito de
diferentes modelos de mundo aplicados à técnica de aprendizado por reforço <span 
class="ec-lmri-12">Q-Learning</span>
sobre a performance do time na posição de defesa. Este trabalho busca encontrar modelos
que facilitem o aprendizado dos agentes na tentativa de interceptar bolas lançadas pelo
adversário e de capturar a bola em posse do adversário. Para alcançar o objetivo foram
desenvolvidos modelos através de diferentes combinações de estados, ações, recompensas
e  métodos  de  implementação.  Os  modelos  foram  treinados  e  avaliados  através  da
ferramenta HFO que cria situações de defesas aleatoriamente agilizando o processo de
treinamento. Os resultados encontrados foram comparados entre si e entre o time base
original (<span 
class="ec-lmri-12">WrightEagleBASE</span>), mas apresentaram desempenho abaixo do esperado, levando
discussões de possíveis falhas no processo.
<!--l. 253--><p class="noindent" ><span 
class="ec-lmbx-12">Palavras-chave</span>:  Futebol  de  Robôs,  Aprendizado  por  reforço,  <span 
class="ec-lmri-12">Q-Learning</span>,  <span 
class="ec-lmri-12">RoboCup</span>
<span 
class="ec-lmri-12">Simulation 2D</span>, Defesa.
                                                                                       <img 
src="main23x.png" alt="PICT" >
<img 
src="main24x.png" alt="PICT" >
                                                                                       
<!--l. 259--><p class="indent" >       <a 
 id="likesection.5"></a><a 
 id="chapterb6.0"></a>
<div class="center" 
>
<!--l. 259--><p class="noindent" >
<!--l. 259--><p class="noindent" ><span 
class="ec-lmb-10x-x-120">ABSTRACT</span></div>
<!--l. 262--><p class="noindent" >This work dwell on the soccer world cup (RoboCup Soccer), which encourages researchers in the
artificial intelligence and robotics fields, being part of the FutVasf2D project that looks for the
development of an official robot soccer team in the 2d simulation league to the UNIVASF,
investigating the effects of different world designs applied to the reinforcement technique
Q-Learning about the performance of the team in defense position. This work tries to find
designs that improve the learning of the agents into trying to intercept balls launched by the
adversaries and capture the ball in the opponent possession. To reach this objective designs
with differents combination of states, actions, rewards and implementation methods were
developed. The designs were trained e evaluated through the HFO tools, which creates
situations of defense randomly making the learning process faster. The found results were
compared between themselves and the original base team (WrightEagleBASE), but showed
performance below the expected, leading to discussions about the possible faults in the
process.
<!--l. 280--><p class="noindent" ><span 
class="ec-lmbx-12">Key-words</span>: <span 
class="ec-lmri-12">Robot Soccer, Reinforcement Learning, Q-Learning, RoboCup Simulation 2D,</span>
<span 
class="ec-lmri-12">Defense</span>.
                                                                                       <img 
src="main25x.png" alt="PICT" >
<img 
src="main26x.png" alt="PICT" >
                                                                                       
<!--l. 291--><p class="indent" >       <a 
 id="lof.0"></a>
<!--l. 292--><p class="noindent" ><span 
class="ec-lmb-10x-x-120">LISTA DE ILUSTRA</span><span 
class="ec-lmb-10x-x-120">Ç</span><span 
class="ec-lmb-10x-x-120">ÕES</span>
       <div class="tableofcontents"><span class="lofToc" >1&#x00A0;<a 
href="#x1-4001r1">Arquitetura geral de um agente.</a></span><br /><span class="lofToc" >2&#x00A0;<a 
href="#x1-12001r2">Representação do tabuleiro
de jogo-da-velha.</a></span><br /><span class="lofToc" >3&#x00A0;<a 
href="#x1-13001r3">Aprendizado por reforço.</a></span><br />
       </div>
                                                                                       <img 
src="main27x.png" alt="PICT" >
<img 
src="main28x.png" alt="PICT" >
                                                                                       
       <a 
 id="lot.0"></a>
<!--l. 301--><p class="noindent" ><span 
class="ec-lmb-10x-x-120">LISTA DE TABELAS</span>
       <div class="tableofcontents">
       </div>
                                                                                       <img 
src="main29x.png" alt="PICT" >
<img 
src="main30x.png" alt="PICT" >
                                                                                       
<span 
class="ec-lmb-10x-x-120">LISTA DE C</span><span 
class="ec-lmb-10x-x-120">ÓDIGOS</span>
       <div class="tableofcontents"><span class="lofToc" >1&#x00A0;<a 
href="#x1-4001r1">Arquitetura geral de um agente.</a></span><br /><span class="lofToc" >2&#x00A0;<a 
href="#x1-12001r2">Representação do tabuleiro
de jogo-da-velha.</a></span><br /><span class="lofToc" >3&#x00A0;<a 
href="#x1-13001r3">Aprendizado por reforço.</a></span><br />
       </div>
<!--l. 322--><p class="indent" >       <a 
 id="likesection.6"></a><a 
 id="chapterb8.0"></a>
                                                                                       <img 
src="main31x.png" alt="PICT" >
<img 
src="main32x.png" alt="PICT" >
                                                                                       
       <h2 class="likechapterHead"><a 
 id="x1-2000"></a>Lista de abreviaturas e siglas</h2>
    CBR         Competição Brasileira de Robótica
    CMAC       Computador Aritmético de Modelo Cerebelar
    DCBD       Descoberta de Conhecimento em Base de Dados
    HAQL       <span 
class="ec-lmri-12">Q-Learning </span>Acelerado Heuristicamente
    HFO         Ofensa de Meio Campo
    KDD        Descoberta de Conhecimento em Base de Dados, do inglês <span 
class="ec-lmri-12">Knowledge</span>
                  <span 
class="ec-lmri-12">Discovery in Databases</span>
    MDP        Processo de Decisão Markoviano
    PIBIC       Programa Institucional de Bolsas de Iniciação Científica
    QRL         Aprendizado por Reforço Qualitativo
    QSR         Raciocínio Espacial Qualitativo
    RPROP     <span 
class="ec-lmri-12">Backpropagation </span>Resiliente
    SARSA      Estado-Ação-Recompensa-Estado-Ação
    UNIVASF   Universidade Federal do Vale do São Francisco
                                                                                       <img 
src="main33x.png" alt="PICT" >
<img 
src="main34x.png" alt="PICT" >
                                                                                       
<!--l. 342--><p class="indent" >       <a 
 id="toc.0"></a>
                                                                                       <img 
src="main35x.png" alt="PICT" >
<img 
src="main36x.png" alt="PICT" >
                                                                                       
       <h2 class="likechapterHead"><a 
 id="x1-3000"></a>Sumário</h2>
       <div class="tableofcontents">
       <span class="chapterToc" >1 <a 
href="#x1-40001" id="QQ2-1-4">Introdução</a></span>
<br />       &#x00A0;<span class="sectionToc" >1.1 <a 
href="#x1-50001.1" id="QQ2-1-6">Motivação</a></span>
<br />       &#x00A0;<span class="sectionToc" >1.2 <a 
href="#x1-60001.2" id="QQ2-1-7">Definição do Problema</a></span>
<br />       &#x00A0;<span class="sectionToc" >1.3 <a 
href="#x1-70001.3" id="QQ2-1-8">Objetivos</a></span>
<br />       &#x00A0;&#x00A0;<span class="subsectionToc" >1.3.1 <a 
href="#x1-80001.3.1" id="QQ2-1-9">Objetivo Geral</a></span>
<br />       &#x00A0;&#x00A0;<span class="subsectionToc" >1.3.2 <a 
href="#x1-90001.3.2" id="QQ2-1-10">Objetivos Específicos</a></span>
<br />       &#x00A0;<span class="sectionToc" >1.4 <a 
href="#x1-100001.4" id="QQ2-1-11">Organização do Trabalho</a></span>
<br />       <span class="chapterToc" >2 <a 
href="#x1-110002" id="QQ2-1-12">Fundamentação Teórica</a></span>
<br />       &#x00A0;<span class="sectionToc" >2.1 <a 
href="#x1-120002.1" id="QQ2-1-13">Trabalhos Relacionados</a></span>
<br />       &#x00A0;<span class="sectionToc" >2.2 <a 
href="#x1-130002.2" id="QQ2-1-15">Aprendizado por Reforço</a></span>
<br />       &#x00A0;&#x00A0;<span class="subsectionToc" >2.2.1 <a 
href="#x1-140002.2.1" id="QQ2-1-17">Q-Learning</a></span>
       </div>*
                                                                                       <img 
src="main37x.png" alt="PICT" >
<img 
src="main38x.png" alt="PICT" >
                                                                                       
                                                                                       <img 
src="main39x.png" alt="PICT" >
<img 
src="main40x.png" alt="PICT" >
                                                                                       
       <h2 class="chapterHead"><span class="titlemark">Capítulo&#x00A0;1</span><br /><a 
 id="x1-40001"></a>Introdução</h2>
<!--l. 6--><p class="noindent" >Como descrito por <span 
class="ec-lmbx-12">??</span>), a <span 
class="ec-lmri-12">RoboCup</span>, a copa do mundo de futebol de robôs, procura fomentar as
pesquisas acerca de robótica e inteligência artificial fornecendo um problema padrão capaz de
avaliar teorias, algoritmos e arquiteturas de agentes. O projeto da <span 
class="ec-lmri-12">RoboCup </span>é dividido em várias
ligas, desde robôs humanoides até a liga de simulação 2D. Proporcionando assim, um ambiente
criativo e desafiador para que novas tecnologias sejam criadas e aplicadas por alunos de
graduação.
<!--l. 13--><p class="indent" >       Segundo <span 
class="ec-lmbx-12">??</span>), a liga de simulação 2D representa um jogo de futebol, em um <span 
class="ec-lmri-12">framework</span>
multiagente, no qual o ambiente é um campo de futebol em duas dimensões e os agentes são os
jogadores, criando a vantagem de livrar os pesquisadores de toda a parte mecânica e eletrônica
para que o foco se volte à análise de dados e construção da estratégia. <span 
class="ec-lmbx-12">??</span>) ainda afirmam
que a principal parte da liga é a modelagem de uma estratégia ou método efetivo
o suficiente para obter performance superior aos adversários. Por fim, os autores
relatam que uma das estratégias com melhores resultados é a utilização de agentes
inteligêntes.
<!--l. 22--><p class="indent" >       De acordo com <span 
class="ec-lmbx-12">??</span>), um agente inteligente é tudo o que pode ser considerado
capaz de perceber seu ambiente por meio de sensores e de agir sobre seu ambiente por
intermédio de atuadores. A Figura <span 
class="ec-lmbx-12">??</span> ilustra a arquitetura clássica de um agente, onde
é possível perceber que o agente interage com o ambiente por meio de sensores e
atuadores.
<!--l. 27--><p class="indent" >       <hr class="figure"><div class="figure" 
>
                                                                                       <img 
src="main41x.png" alt="PICT" >
<img 
src="main42x.png" alt="PICT" >
                                                                                       
<a 
 id="x1-4001r1"></a>
                                                                                       <img 
src="main43x.png" alt="PICT" >
<img 
src="main44x.png" alt="PICT" >
                                                                                       
<br /> <div class="caption" 
><span class="id">Figura&#x00A0;1: </span><span  
class="content">Arquitetura geral de um agente.</span></div><!--tex4ht:label?: x1-4001r1 -->
<div class="center" 
>
<!--l. 27--><p class="noindent" >
<!--l. 27--><p class="noindent" ></div>
<div class="legend"><span 
class="ec-lmbx-12">Fonte: </span>(<span 
class="ec-lmbx-12">??</span>)</div>
                                                                                       <img 
src="main45x.png" alt="PICT" >
<img 
src="main46x.png" alt="PICT" >
                                                                                       
<!--l. 27--><p class="indent" >       </div><hr class="endfigure">
<!--l. 29--><p class="indent" >       No domínio da liga de simulação 2D da <span 
class="ec-lmri-12">RoboCup</span>, o ambiente é composto pela
representação 2D do campo, as traves, a bola e os jogadores. Esse ambiente é captado pelos
agentes através dos seus sensores classificados como físicos, visuais ou acústicos, que,
respectivamente, servem para perceber o estado do agente, referente ao vigor, velocidade e
posição; as posições e velocidades dos demais objetos no ambiente, incluindo jogadores e
mensagens oriundas dos aliados. Os atuadores dos agentes correspondem aos mecanismos de
movimento, rotação e chute (<span 
class="ec-lmbx-12">??</span>). Um dos aspectos que torna o desenvolvimento de
agentes inteligentes para jogos de futebol de robôs desafiador é a característica de seu
ambiente:
      <ul class="itemize1">
      <li class="itemize"><span 
class="ec-lmbx-12">Parcialmente observ</span><span 
class="ec-lmbx-12">ável</span>: O ambiente para um agente jogador de futebol de robôs
      é parcialmente observável, pois os seus sensores são imprecisos e também porque
      partes do estado estão simplesmente ausentes nos dados do sensor (<span 
class="ec-lmbx-12">??</span>);
      </li>
      <li class="itemize"><span 
class="ec-lmbx-12">Estoc</span><span 
class="ec-lmbx-12">ástico</span>: De acordo com <span 
class="ec-lmbx-12">??</span>), se o próximo estado do ambiente é completamente
      determinado pelo estado atual e pela ação executada pelo agente, dizemos que o
      ambiente é determinístico; caso contrário, ele é estocástico. Desta forma, o ambiente
      para um agente jogador de futebol de robôs é estocástico, pois o próximo estado
      não depende apenas da sua ação;
      </li>
      <li class="itemize"><span 
class="ec-lmbx-12">Epis</span><span 
class="ec-lmbx-12">ódico</span>: Em um ambiente de tarefa episódico, a experiência do agente é dividida
      em episódios atômicos (<span 
class="ec-lmbx-12">??</span>). Cada episódio consiste na percepção do agente, e depois
      na execução de uma única ação. É crucial que o episódio seguinte não dependa das
      ações executadas em episódios anteriores. Desta forma, o ambiente de tarefa para
      o agente jogador de futebol de robôs é episódico;
      </li>
      <li class="itemize"><span 
class="ec-lmbx-12">Din</span><span 
class="ec-lmbx-12">âmico</span>:  Ambientes  estáticos  são  fáceis  de  manipular,  porque  o  agente  não
      precisa continuar a observar o mundo enquanto está decidindo sobre a realização
      de uma ação, nem precisa se preocupar com a passagem do tempo. No entanto, o
      ambiente de tarefa para o agente jogador de futebol de robôs é dinâmico, pois o
      ambiente pode ser alterado enquanto o agente está deliberando (<span 
class="ec-lmbx-12">??</span>);
      </li>
      <li class="itemize"><span 
class="ec-lmbx-12">Multiagente</span>:  Um  ambiente  ainda  pode  ser  classificado  como  agente  único  e
      multiagente. Agente único não existem outros agentes que utilizam a mesma medida
      de  performance,  como  exemplo  podemos  citar:  um  agente  que  resolve  um  jogo
                                                                                       <img 
src="main47x.png" alt="PICT" >
<img 
src="main48x.png" alt="PICT" >
                                                                                       
      de palavras cruzadas sozinho está claramente em um ambiente de agente único,
      enquanto que um agente jogador de futebol de robôs está em ambiente multiagente,
      tanto cooperativo (agentes do mesmo time) como também competitivos (agentes do
      time adversário) (<span 
class="ec-lmbx-12">??</span>).
      </li></ul>
       <h3 class="sectionHead"><span class="titlemark">1.1   </span> <a 
 id="x1-50001.1"></a>Motivação</h3>
<!--l. 72--><p class="noindent" >FutVasf2D é um projeto que nasceu em 2017 com auxílio do Programa Institucional de Bolsas
de Iniciação Científica (PIBIC) para o desenvolvimento de um time oficial de futebol robôs
para Universidade Federal do Vale do São Francisco (UNIVASF) na categoria de simulação 2D.
O primeiro trabalho do projeto teve como objetivo o desenvolvimento de um módulo de
decisão para o momento do chute a gol. O projeto está tendo continuidade com, além
do presente trabalho, outro PIBIC que visa o desenvolvimento de um módulo de
passes.
<!--l. 79--><p class="indent" >       Durante o processo de desenvolvimento deste trabalho almeja-se por em prática
conhecimentos tratados forma téorica na área de aprendizado de máquinas enfrentando o
desafio da aplicação das técnicas tratadas em um ambiente complexo e prático. A técnica de
aprendizado por reforço aplicada neste trabalho é a <span 
class="ec-lmri-12">Q-Learning </span>devido ao fato desta ser uma
técnica comum e já utilizada por diversos trabalhos na área.
<!--l. 85--><p class="indent" >       Com o término deste trabalho espera-se a participação do time desenvolvido na
Competição Brasileira de Robótica (CBR) como meio de divulgar o projeto e angariar
interessados no desenvolvimento de novas estratégias e melhoramento das já desenvolvidas no
meio acadêmico da UNIVASF.
<!--l. 90--><p class="noindent" >
       <h3 class="sectionHead"><span class="titlemark">1.2   </span> <a 
 id="x1-60001.2"></a>Definição do Problema</h3>
<!--l. 92--><p class="noindent" >De acordo com <span 
class="ec-lmbx-12">??</span>), uma partida de futebol apresenta situações que exigem dos jogadores a
tomada de decisão acerca da ação a ser tomada. Quando um jogador está com a bola, por
exemplo, ele deve decidir se deve passar a bola para um aliado, driblar um adversário ou chutar
a bola para o gol. Todas essas ações são decididas através de avaliações do conhecimento acerca
da situação atual do jogo. O mesmo pode ser afirmado para situações onde o time
adversário está com posse de bola, neste caso, os jogadores devem sempre tomar a melhor
decisão com base no estado atual para evitar sofrer gol e recuperar a posse de bola do
adversário.
<!--l. 100--><p class="indent" >       Alguns times que competem ou competiram em campeonatos da liga fornecem
                                                                                       <img 
src="main49x.png" alt="PICT" >
<img 
src="main50x.png" alt="PICT" >
                                                                                       
código-fonte de times, chamados de times bases, a fim de facilitar o desenvolvimento de novos
times para novos competidores. É o caso dos times HELIOS, do Japão, que fornece o time base
Agent2D<a 
href="#Hfootnote.\theHHfootnote"  ><span class="footnote-mark"><a 
href="main2.html#fn1x2"><sup class="textsuperscript">1</sup></a></span></a><a 
 id="x1-6001f1"></a> ou o WrightEagle, da China,
que fornece o WrightEagleBASE<a 
href="#Hfootnote.\theHHfootnote"  ><span class="footnote-mark"><a 
href="main3.html#fn2x2"><sup class="textsuperscript">2</sup></a></span></a><a 
 id="x1-6002f2"></a>,
utilizado neste projeto. Tais times são desenvolvidos com estratégias simples para que possam
ser melhoradas pelas novas equipes.
<!--l. 106--><p class="indent" >       <span 
class="ec-lmbx-12">??</span>) afirmam que o uso de técnicas de aprendizagem para o desenvolvimento da
capacidade defensiva de um time vêm sendo quase negligenciadas, de forma que as pesquisas
focam, em sua maioria, em tarefas como a de passe, chute a gol ou posicionamento do jogador
com posse de bola.
<!--l. 110--><p class="indent" >       Ainda segundo <span 
class="ec-lmbx-12">??</span>), estratégias de defesa são divididas em 2 partes, o posicionamento
dos jogadores de forma a melhor interceptar passes ou tentativas de chute ou marcar oponentes,
e o de atacar o jogador com posse de bola a fim de obtê-la.
<!--l. 114--><p class="indent" >       Neste trabalho, serão levadas em conta ambas as facetas das estratégias de defesa, de
forma que o problema proposto pode ser definido como a identificação de melhor
ação a ser tomada, utilizando dados obtidos do ambiente através de seus sensores,
entre:
      <ul class="itemize1">
      <li class="itemize">Mover-se para uma melhor posição;
      </li>
      <li class="itemize">Marcar um jogador adversário;
      </li>
      <li class="itemize">Interceptar a bola e
      </li>
      <li class="itemize">Bloquear avanço do jogador com posse de bola.</li></ul>
       <h3 class="sectionHead"><span class="titlemark">1.3   </span> <a 
 id="x1-70001.3"></a>Objetivos</h3>
                                                                                       <img 
src="main51x.png" alt="PICT" >
<img 
src="main52x.png" alt="PICT" >
                                                                                       
<!--l. 127--><p class="noindent" >
       <h4 class="subsectionHead"><span class="titlemark">1.3.1   </span> <a 
 id="x1-80001.3.1"></a>Objetivo Geral</h4>
<!--l. 129--><p class="noindent" >Investigar performance de modelos no desenvolvimento de módulo responsável por tomar
decisões voltadas para defesa para o time de futebol de robôs da liga de simulação 2D
FutVasf2D utilizando a técnica de aprendizado por reforço <span 
class="ec-lmri-12">Q-Learning</span>.
<!--l. 133--><p class="noindent" >
       <h4 class="subsectionHead"><span class="titlemark">1.3.2   </span> <a 
 id="x1-90001.3.2"></a>Objetivos Específicos</h4>
<!--l. 136--><p class="noindent" >
      <dl class="enumerate-enumitem"><dt class="enumerate-enumitem">
   1. </dt><dd 
class="enumerate-enumitem">Desenvolver modelos aptos a representar o ambiente do futebol de robôs da liga de
      simulação 2D em algoritmos de aprendizado por reforço aplicados na solução do
      problema proposto;
      </dd><dt class="enumerate-enumitem">
   2. </dt><dd 
class="enumerate-enumitem">Implementar   os   modelos   propostos   no   algoritmo   <span 
class="ec-lmri-12">Q-Learning   </span>no   time
      <span 
class="ec-lmri-12">WrightEagleBASE</span>, usado como base para o FutVasf2D;
      </dd><dt class="enumerate-enumitem">
   3. </dt><dd 
class="enumerate-enumitem">Treinar os times implementados através da execução de partidas com situações
      controladas;
      </dd><dt class="enumerate-enumitem">
   4. </dt><dd 
class="enumerate-enumitem">Comparar resultados e identificar e falhas e melhorias.</dd></dl>
<!--l. 147--><p class="noindent" >
       <h3 class="sectionHead"><span class="titlemark">1.4   </span> <a 
 id="x1-100001.4"></a>Organização do Trabalho</h3>
<!--l. 149--><p class="noindent" >Além do presente capítulo, Introdução, estão presentes neste trabalho 3 capítulos,
Fundamentação Teórica (<a 
href="#x1-110002">2<!--tex4ht:ref: sec:fundamentation --></a>), Materiais e Métodos (<span 
class="ec-lmbx-12">??</span>), Resultados (<span 
class="ec-lmbx-12">??</span>), Conclusão (<span 
class="ec-lmbx-12">??</span>) e
Trabalhos Futuros (<span 
class="ec-lmbx-12">??</span>).
<!--l. 153--><p class="indent" >       O Capítulo <a 
href="#x1-110002">2<!--tex4ht:ref: sec:fundamentation --></a> fundamenta teoricamente o trabalho apresentando na Seção <a 
href="#x1-120002.1">2.1<!--tex4ht:ref: trabalhosRelacionados --></a> trabalhos
relacionados que reforçam a metodologia por trás da execução deste, a Seção <a 
href="#x1-130002.2">2.2<!--tex4ht:ref: aprendizado --></a> explana o
conceito e funcionamento do aprendizado por reforço e, na Subseção <a 
href="#x1-140002.2.1">2.2.1<!--tex4ht:ref: qlearning --></a>, faz o
mesmo com o algoritmo <span 
class="ec-lmri-12">Q-Learning</span>, ponto fundamental para o desenvolvimento deste
trabalho.
                                                                                       <img 
src="main53x.png" alt="PICT" >
<img 
src="main54x.png" alt="PICT" >
                                                                                       
<!--l. 159--><p class="indent" >       O Capítulo <span 
class="ec-lmbx-12">??</span> explica o processo proposto para o desenvolvimento dos modelos e coleta
de resultados. Na Seção <span 
class="ec-lmbx-12">??</span> é exposto o conceito de modelagem do ambiente e seus desafios. A
Seção <span 
class="ec-lmbx-12">??</span> explica o processo de implementação e as variações aplicadas, enquanto a
Seção <span 
class="ec-lmbx-12">??</span>, é acerca da metodologia experimental, como o treinamento e a forma de
avaliação.
<!--l. 165--><p class="indent" >       O Capítulo <span 
class="ec-lmbx-12">??</span> apresenta a descrição dos modelos desenvolvidos, a fundamentação de
cada um e os resultados obtidos em seus respectivos treinamentos.
<!--l. 168--><p class="indent" >       No Capítulo final <span 
class="ec-lmbx-12">??</span> constam as devidas conclusões tomadas a partir do processo de
desenvolvimento deste trabalho e dos resultados obtidos pelos experimentos, procurando
responder indagações oriundas da análise dos dados do capítulo anterior. Também é
trabalhado na Seção <span 
class="ec-lmbx-12">??</span>, o traçado de uma continuidade para o projeto iniciado apresentando
propostas de novas investigações e o caminho a ser tomado.
                                                                                       <img 
src="main55x.png" alt="PICT" >
<img 
src="main56x.png" alt="PICT" >
                                                                                       
       <h2 class="chapterHead"><span class="titlemark">Capítulo&#x00A0;2</span><br /><a 
 id="x1-110002"></a>Fundamentação Teórica</h2>
       <h3 class="sectionHead"><span class="titlemark">2.1   </span> <a 
 id="x1-120002.1"></a>Trabalhos Relacionados</h3>
<!--l. 8--><p class="noindent" >Diversos trabalhos foram redigidos utilizando aprendizado por reforço no domínio da <span 
class="ec-lmri-12">RoboCup</span>
<span 
class="ec-lmri-12">Soccer Simulation 2D</span>, cada um desses trabalhos apresentam modelagens diferentes para o
mundo.
<!--l. 11--><p class="indent" >       A modelagem de mundo é a forma como o ambiente é representado, ou simulado. Um
agente inteligente precisa constantemente entender como está o ambiente no qual ele está
inserido, por isso é importante a definição da representação do mundo de forma que melhor se
adéque ao processo de aprendizado do agente.
<!--l. 16--><p class="indent" >       Um agente empregado no aprendizado do jogo-da-velha, pode ser utilizado com exemplo.
O agente precisa entender como o jogo está sendo encaminhado para que o mesmo possa decidir
em que posição jogar, porém, o agente isolado não tem acesso ao estado do jogo e precisa de
uma representação do mesmo para que possa entende-lo. Uma abordagem válida é numerar o
tabuleiro conforme a Figura <span 
class="ec-lmbx-12">??</span> e atrelar a cada célula um valor que varia de acordo com o que
é marcado na mesma, uma escolha razoável é atribuir o valor &#8220;<span 
class="lmsy-10x-x-120">-</span><span 
class="rm-lmr-12">1</span>&#8221; à células preenchidas pelo
jogador adversário, &#8220;<span 
class="rm-lmr-12">0</span>&#8221; à células não preenchidas e &#8220;<span 
class="rm-lmr-12">1</span>&#8221; às preenchidas pelo próprio agente.
Nota-se que com essa representação o agente consegue perceber em que situação
se encontra e assim escolher uma ação. Podemos chamar cada uma das células de
variável, já que nelas se encontram os valores que determinam qual o estado atual do
ambiente.
<!--l. 27--><p class="indent" >       <hr class="figure"><div class="figure" 
>
                                                                                       <img 
src="main57x.png" alt="PICT" >
<img 
src="main58x.png" alt="PICT" >
                                                                                       
<a 
 id="x1-12001r2"></a>
                                                                                       <img 
src="main59x.png" alt="PICT" >
<img 
src="main60x.png" alt="PICT" >
                                                                                       
<br /> <div class="caption" 
><span class="id">Figura&#x00A0;2: </span><span  
class="content">Representação do tabuleiro de jogo-da-velha.</span></div><!--tex4ht:label?: x1-12001r2 -->
<div class="center" 
>
<!--l. 27--><p class="noindent" >
<!--l. 27--><p class="noindent" ></div>
<div class="legend"><span 
class="ec-lmbx-12">Fonte: </span>O Autor</div>
                                                                                       <img 
src="main61x.png" alt="PICT" >
<img 
src="main62x.png" alt="PICT" >
                                                                                       
<!--l. 27--><p class="indent" >       </div><hr class="endfigure">
<!--l. 29--><p class="indent" >       É fácil encontrar exemplos de problemas inseridos em ambientes que são mais complexos
que o citado. Para esses problemas é comum encontrar variáveis de valores contínuos, nesses
casos é necessário pensar em como representar as variáveis de forma discreta para que seja
possível se obter um número finito de estados. Alguns dos trabalhos citados adiante
demonstram maneiras de interpretar os dados do ambiente para que possa ser aplicada técnica
de aprendizado.
<!--l. 35--><p class="indent" >       <span 
class="ec-lmbx-12">??</span>) desenvolveram jogadores de defesa com comportamento agressivo, ou seja,
jogadores que interferem e tentam tomar a bola do jogador adversário, utilizando
aprendizado por reforço. O algoritmo desenvolvido foi chamado de NeuroHassle e
utiliza redes neurais e uma abstração do mundo em 9 dimensões, distância entre o
jogador e oponente com posse de bola; vetor de velocidade do jogador; velocidade
absoluta do oponente com posse de bola; vetor de posicionamento da bola; ângulo do
corpo do jogador em relação ao oponente com posse de bola; ângulo do corpo do
adversário em relação ao gol aliado; ângulo definido pela posição do gol aliado, a posição
do oponente com posse de bola e a posição do jogador. O treinamento foi feito de
forma episódica em partidas de competições anteriores. O time gerado obteve uma
taxa de falha menor que 20% e, em competição, cerca de 30 tomadas de bola por
partida.
<!--l. 46--><p class="indent" >       Uma variação de redes neurais com a utilização de <span 
class="ec-lmri-12">backpropagation</span>, chamada de
<span 
class="ec-lmri-12">Backpropagation </span>Resiliente (RPROP, do inglês <span 
class="ec-lmri-12">Resilient Backpropagation</span>), foi utilizada por <span 
class="ec-lmbx-12">??</span>)
no treinamento de jogadores sem posse de bola ao mesmo tempo que a escolha de ações do
jogador com posse de bola. Para isto foi feita uma discretização do domínio em um
Processo de Decisão Markoviano (MDP, do inglês <span 
class="ec-lmri-12">Markovian Decision Process</span>) que
destacou as variáveis posição e velocidade da bola e a posição de todos os atacantes
aliados e defensores adversários, além das possíveis ações, que para os jogadores
com posse de bola consistem em passar a bola diretamente para um aliado, lançar
a bola de forma que um aliado possa interceptar a bola antes de um adversário e
driblar, já para os jogadores sem posse de bola, é possível se mover em 8 direções a
partir do local atual ou a partir do local inicial ou mover-se para o local inicial. Este
trabalho resultou no time vencedor do ano de 2007 da liga de simulação 2D da <span 
class="ec-lmri-12">RoboCup</span>
<span 
class="ec-lmri-12">Soccer</span>.
<!--l. 59--><p class="indent" >       A técnica <span 
class="ec-lmri-12">Q-Learning </span>é aplicada em alguns trabalhos, como o de <span 
class="ec-lmbx-12">??</span>), que utiliza desta
técnica a fim de obter um sistema de tomada de decisões focado no ataque. Nesse trabalho, os
estados foram discretizados em 7 estados, caracterizados como quando é possível lançar a
bola para um aliado melhor posicionado; um adversário mais próximo está atrás do
jogador com posse de bola; o adversário mais próximo está a mais de 7 metros; o
adversário mais próximo está a uma distância entre 7 e 6 metros; o adversário mais
                                                                                       <img 
src="main63x.png" alt="PICT" >
<img 
src="main64x.png" alt="PICT" >
                                                                                       
próximo está a uma distância entre 6 e 5 metros; o adversário mais próximo está a uma
distância menor de 4 metros e há uma linha direta entre o jogador com posse de
bola e o gol adversário. Além disso, 7 ações podem ser tomadas, são elas, passar a
bola para um jogador mais próximo do gol adversário, avançar devagar, avançar
rapidamente, passar a bola para um jogador próximo, driblar, segurar a bola ou chutar ao
gol.
<!--l. 71--><p class="indent" >       O modelo foi treinado e testado através da execução de conjuntos de 10 partidas. O
conjunto de treinamento se deu contra o time base do Helios apenas na situação de posse de
bola. Após o treinamento, o time foi tuilizado no primeiro conjunto de testes contra o time
base do Helios, além disso, outro foiexecutado entre Helios e o PetSoccer, além de
um entre o time treinado e o PetSoccer para comparação, e por fim outro contra o
time chinês WrightEagles. No primeiro conjunto foram realizados 6 gols contra o
Helios, o Helios executou 47 gols contra o PetSoccer, o time treinado conseguiu 61
gols contra o PetSoccer e 6 contra o WrightEagles. Mostrando que a utilização do
<span 
class="ec-lmri-12">Q-Learning </span>resultou numa melhoria, ganhando a maioria dos jogos. A mesma técnica foi
utilizada no time brasileiro GPR-2D de 2012, descrito em <span 
class="ec-lmbx-12">??</span>), com o objetivo de treinar
a tomada de decisão para quando o jogador estiver com posse de bola. O time foi
classificado e chegou a segunda fase do campeonato mundial da <span 
class="ec-lmri-12">RoboCup Soccer Simulation</span>
<span 
class="ec-lmri-12">2D</span>.
<!--l. 83--><p class="indent" >       <span 
class="ec-lmbx-12">?? </span>(<span 
class="ec-lmbx-12">?? </span>e <span 
class="ec-lmbx-12">??</span>), utilizam <span 
class="ec-lmri-12">Q-Learning </span>com uma abstração do conjunto de agentes como um
grande agente e o conjunto de suas ações como uma ação deste grande agente chamando os dois
algoritmos criados de <span 
class="ec-lmri-12">Regional Cooperative Q-Learning </span>e <span 
class="ec-lmri-12">Sparse Cooperative Q-Learning</span>
(<span 
class="ec-lmri-12">Q-Learning </span>Cooperativo Regional e <span 
class="ec-lmri-12">Q-Learning </span>Cooperativo Esparso). Nesses trabalhos foram
identificados estados onde não é necessário que as ações sejam decididas em conjunto (não
coordenados) e onde é (coordenados), utilizando uma abordagem baseado em campos
potenciais. Na abordagem regional foram encontrados 5012 estados coordenados e 34390
não coordenados, já na abordagem esparsa, foram 4720 coordenados e 34682 não
coordenados. O modelo foi treinado com uma simplificação de partidas com dois
agentes defendendo e um adversário atacando e a região (reduzida para 40 x 20)
dividida em 200 células. Foram rodados experimentos com 200000 e 500000 episódios
comparando a técnica aplicada com a técnica MDP e a linear independente. Para 200000
episódios foi obtida uma média de tempo de captura de 6,57 segundos para a abordagem
regional e 6,49 para a esparsa, mostrando-se o melhor dentre 3 métodos enquanto com
500000 episódios apresenta uma média de 6,49 segundos na abordagem regional e 6,40
na abordagem esparsa, maior apenas que o do método MDP que apresentou 6,23
segundos.
<!--l. 99--><p class="indent" >       Alguns trabalhos utilizam variações do <span 
class="ec-lmri-12">Q-Learning</span>, como <span 
class="ec-lmbx-12">??</span>), que desenvolveu um
módulo de drible unindo Computador Aritmético de Modelo Cerebelar (CMAC, do inglês
                                                                                       <img 
src="main65x.png" alt="PICT" >
<img 
src="main66x.png" alt="PICT" >
                                                                                       
<span 
class="ec-lmri-12">Cerebellar Model Arithmetic Computer</span>) e Estado-Ação-Recompensa-Estado-Ação (SARSA, do
inglês <span 
class="ec-lmri-12">State-Action-Reward-State-Action</span>), este último algoritmo foi concebido como <span 
class="ec-lmri-12">Q-Learning</span>
conexionista modificado. Neste trabalho a tarefa de drible foi mapeada como um problema
episódico adequado para o trabalho com aprendizado por reforço onde foram destacadas
duas ações primitivas, segurar a bola e driblar (que consiste em girar o corpo a uma
determinada angulação, chutar a bola a uma distância determinada e correr para
interceptá-la) e as seguintes variáveis para compor um estado para o jogador que
deverá driblar, o ângulo global de um objeto, o ângulo relativo entre dois objetos, a
distância entre dois objetos, a altura e largura do campo e se o jogador está perto do
topo ou da parte mais baixa do campo. O treinamento do modelo criado foi feito em
5 experimentos de 50000 episódios nos quais o jogadores não podiam recuperar a
<span 
class="ec-lmri-12">stamina </span>por si só utilizando o ambiente padrão e um ambiente de treino de 20 x 20.
Para testar foram executados 10000 cenários com configurações aleatórias. No fim
do treinamento foi obtido um número de vitórias de cerca de 53% e 58% dos testes
pós-treino.
<!--l. 115--><p class="indent" >       O SARSA também foi empregado em <span 
class="ec-lmbx-12">??</span>) em conjunto com o Raciocínio Espacial
Qualitativo (QSR, do inglês <span 
class="ec-lmri-12">Qualitative Spatial Reasoning</span>), chamando o resultado de
Aprendizado por Reforço Qualitativo (QRL, do inglês <span 
class="ec-lmri-12">Qualitative Reinforcement Learning</span>) e
aplicou no desenvolvimento de um mecanismo de ataque para um time de simulação 2D. Para
alcançar o objetivo foi utilizado o formalismo eOPRAm para discretizar o mundo em regiões
qualitativas pelo qual foram identificadas um total de 14 variáveis, são elas a posição e
orientação do jogador, a distância e ângulo do jogador em relação à bola, se o jogador pode
ou não chutar, distância e ângulo do jogador em relação ao gol adversário, maior
ângulo aberto para o gol, distância entre os aliados e o adversário mais próximo,
maior ângulo aberto para passe para cada aliado, numeração de cada aliado e de cada
adversário.
<!--l. 126--><p class="indent" >       As possíveis ações foram definidas como driblar, chutar e passar a bola para um aliado
especifico. O treinamento foi realizado utilizando a ferramenta Ofensa de Meio Campo (HFO,
do inglês <span 
class="ec-lmri-12">Half-Field Offense</span>) 30 vezes de forma independente com 1000 episódios em 3
situações diferentes, a primeira com apenas um jogador atacando sem nenhuma defesa, a
segunda com apenas o atacante e o goleiro e a terceira com o atacante, o goleiro e um zagueiro.
Foi realizada uma comparação da técnica utilizada com a abordagem quantitativa utilizando
análise de variância. Na primeira situação o resultado foi de uma média de 99,3% de gols se
mostrando melhor que a quantitativa com certeza de 95%, enquanto a segunda e terceira
situação se mostraram melhor com margem de 1% com 81,2% e 47,6% contra 78,3% e 40,6%
respectivamente.
<!--l. 136--><p class="indent" >       Além dessas variações, algumas foram empregadas com o intuito de acelerar o
aprendizado, este foi o caso de <span 
class="ec-lmbx-12">??</span>) e <span 
class="ec-lmbx-12">??</span>). O primeiro, utilizou a chamada <span 
class="ec-lmri-12">Q-Learning </span>Acelerado
                                                                                       <img 
src="main67x.png" alt="PICT" >
<img 
src="main68x.png" alt="PICT" >
                                                                                       
Heuristicamente (HAQL, do inglês <span 
class="ec-lmri-12">Heuristically Accelerated Q-Learning</span>) para treinar um
goleiro e jogador de defesa. Para tanto, foi identificada uma heurística aplicável ao domínio e
implementada no algoritmo <span 
class="ec-lmri-12">Q-Learning </span>ao mesmo tempo que é feita a discretização do mundo.
As variáveis de estado para o algoritmo foram a posição dos agentes e da bola numa grade e
a direção para a qual o agente está virado definida por norte, sul, leste ou oeste.
As ações que podem ser tomadas são mudar a direção para a qual o agente está
virado para um objeto especifico, mover em direção a bola, mover-se com a bola,
passar a bola para o goleiro, chutar a bola para longe do gol mover-se para perto de
um oponente. Foram executadas 10 sessões de treinamento para o algoritmo criado,
<span 
class="ec-lmri-12">Q-Learning </span>puro e a função heurística pura com 100 episódios cada, onde cada episódio
consiste de uma partida de 3000 ciclos. Foi analisada a curva de aprendizado e o
número de gols sofridos para cada algoritmo. E utilizado o teste <span 
class="ec-lmri-12">t-student </span>que validou a
hipótese de que a aplicação de heurística no <span 
class="ec-lmri-12">Q-Learning </span>acelera o aprendizado com
nível de confiança de 95% e uma taxa de gols sofridos menor que as demais opções
testadas.
<!--l. 152--><p class="indent" >       Já <span 
class="ec-lmbx-12">??</span>), utilizam heurística baseada em casos para acelerar o <span 
class="ec-lmri-12">Minmax-Q</span>, que também é
uma variação do <span 
class="ec-lmri-12">Q-Learning</span>, para melhorar a tomada de decisões. Os casos destacado para
utilização no algoritmo foram o de que caso o jogador esteja com a bola e não exista adversário
no caminho, avança para o gol; se houver um adversário bloqueando, mova para
cima ou para baixo; se houver um aliado mais próximo do gol, passa a bola e se o
adversário estiver com a bola e o jogador estiver próximo, fica na frente do adversário. O
treinamento foi feito com 20000 jogos de 10 tentativas em um domínio chamado &#8220;futebol
expandido de Littman&#8221;. Chegou-se ao resultado de que a solução encontrada não é
ótima, mas a hipótese de que o aprendizado é acelerado foi comprovado, através do
teste de <span 
class="ec-lmri-12">t-student </span>e uma comparação dos resultados das partidas e das curvas de
aprendizado.
<!--l. 163--><p class="indent" >       Diante dos exemplos apresentados e considerando a natureza do problema, fica decidido
utilizar uma técnica de aprendizado por reforço, mais especificamente <span 
class="ec-lmri-12">Q-Learning</span>, para o
treinamento de um modelo de tomada de decisões de defesa do time em desenvolvimento,
<span 
class="ec-lmri-12">FutVasf2D</span>.
       <h3 class="sectionHead"><span class="titlemark">2.2   </span> <a 
 id="x1-130002.2"></a>Aprendizado por Reforço</h3>
<!--l. 169--><p class="noindent" >Segundo <span 
class="ec-lmbx-12">??</span>), aprendizado por reforço é uma técnica de aprendizado intermediária, entre
supervisionado e não supervisionado, onde o agente precisa realizar mais previsões durante os
&#8220;testes&#8221;.
<!--l. 173--><p class="indent" >       <span 
class="ec-lmbx-12">??</span>) afirmam que aprendizado por reforço é necessário quando não há um bom
&#8221;professor&#8220; para a tarefa a ser executada. Apontam ainda que, através de ações aleatórias, o
                                                                                       <img 
src="main69x.png" alt="PICT" >
<img 
src="main70x.png" alt="PICT" >
                                                                                       
modelo é capaz de &#8220;aprender&#8221; a realizar predições sobre o ambiente. Logo, o papel do
aprendizado por reforço é, utilizando-se de recompensas, encontrar uma função eficaz para o
agente.
<!--l. 178--><p class="indent" >       Uma analogia apontada por <span 
class="ec-lmbx-12">??</span>) é a que compara o aprendizado por reforço com a forma
que seres vivos aprendem. Animais são capazes de perceber quando especificas sensações são
recompensas positivas ou negativas e essa é uma característica que facilita o adestramento de
cães, por exemplo, no qual o cão consegue entender que fez algo positivo através de
recompensas como petiscos ou carinho ou negativo como a repreensão em determinado tom de
voz.
<!--l. 185--><p class="indent" >
                           <!--l. 185--><p class="noindent" ><span 
class="ec-lmr-10">Aprendizado por refor</span><span 
class="ec-lmr-10">ço </span><span 
class="ec-lmr-10">é aprender o que fazer - como mapear situa</span><span 
class="ec-lmr-10">ç</span><span 
class="ec-lmr-10">ões em</span>
                           <span 
class="ec-lmr-10">a</span><span 
class="ec-lmr-10">ç</span><span 
class="ec-lmr-10">ões - de forma a maximizar uma recompensa num</span><span 
class="ec-lmr-10">érica. Aquele que aprende</span>
                           <span 
class="ec-lmr-10">n</span><span 
class="ec-lmr-10">ão </span><span 
class="ec-lmr-10">é informado quais a</span><span 
class="ec-lmr-10">ç</span><span 
class="ec-lmr-10">ões tomar, no entanto, deve descobrir quais</span>
                           <span 
class="ec-lmr-10">a</span><span 
class="ec-lmr-10">ç</span><span 
class="ec-lmr-10">ões resultam em uma maior recompensa tentando execut</span><span 
class="ec-lmr-10">á-las.</span>
                           <span 
class="ec-lmr-10">(</span><span 
class="ec-lmbx-10">??</span><span 
class="ec-lmr-10">)</span>
<!--l. 191--><p class="indent" >       Segundo <span 
class="ec-lmbx-12">??</span>), existem alguns elementos que devem ser conceituados em prol de melhor
entender o aprendizado por reforço, são eles, política, sinal de recompensa, função valor e
modelo de ambiente.
      <ul class="itemize1">
      <li class="itemize"><span 
class="ec-lmbx-12">Pol</span><span 
class="ec-lmbx-12">ítica </span>é o que define a forma como o agente deve se comportar em determinado
      momento. É um mapeamento entre os possíveis estados do ambiente e ações.
      </li>
      <li class="itemize"><span 
class="ec-lmbx-12">Sinal de recompensa </span>é um valor numérico que define o quão determinada ação
      afeta de forma positiva ou negativa na tentativa de alcançar o objetivo.
      </li>
      <li class="itemize"><span 
class="ec-lmbx-12">Fun</span><span 
class="ec-lmbx-12">ç</span><span 
class="ec-lmbx-12">ão de valor </span>se refere ao que o agente espera acumular de recompensa no
      futuro. É como a função de recompensa, porém, ao invés de um valor imediato,
      retorna um valor para um intervalo de tempo maior.
      </li>
      <li class="itemize"><span 
class="ec-lmbx-12">Modelo de ambiente </span>é uma simulação do ambiente que permite que sejam feitas
      interferências em como o ambiente se comportará. Modelos são usados para prever
      como estará o ambiente sem precisar altera-lo de fato para melhor planejar as ações.</li></ul>
                                                                                       <img 
src="main71x.png" alt="PICT" >
<img 
src="main72x.png" alt="PICT" >
                                                                                       
<!--l. 211--><p class="indent" >       A Figura <span 
class="ec-lmbx-12">??</span> resume o funcionamento do aprendizado por reforço como o já
dito.
<!--l. 213--><p class="indent" >       <hr class="figure"><div class="figure" 
>
                                                                                       <img 
src="main73x.png" alt="PICT" >
<img 
src="main74x.png" alt="PICT" >
                                                                                       
<a 
 id="x1-13001r3"></a>
                                                                                       <img 
src="main75x.png" alt="PICT" >
<img 
src="main76x.png" alt="PICT" >
                                                                                       
<br /> <div class="caption" 
><span class="id">Figura&#x00A0;3: </span><span  
class="content">Aprendizado por reforço.</span></div><!--tex4ht:label?: x1-13001r3 -->
<div class="center" 
>
<!--l. 213--><p class="noindent" >
<!--l. 213--><p class="noindent" ></div>
<div class="legend"><span 
class="ec-lmbx-12">Fonte: </span>O Autor</div>
                                                                                       <img 
src="main77x.png" alt="PICT" >
<img 
src="main78x.png" alt="PICT" >
                                                                                       
<!--l. 213--><p class="indent" >       </div><hr class="endfigure">
<!--l. 215--><p class="indent" >       <span 
class="ec-lmbx-12">??</span>) descrevem como o aprendizado por reforço pode variar, tanto em relação ao tipo de
ambiente quanto ao agente:
      <ul class="itemize1">
      <li class="itemize">O ambiente pode ser acessível ou inacessível. No primeiro caso o agente consegue
      detectar o estado do ambiente através de sensores, enquanto que em ambientes
      inacessíveis o agente precisa manter e atualizar um estado interno.
      </li>
      <li class="itemize">O agente pode saber <span 
class="ec-lmri-12">a priori  </span>sobre os estados do ambiente e como suas ações
      podem interferir no mesmo ou pode ter que aprender essas informações durante o
      treinamento.
      </li>
      <li class="itemize">Recompensas  podem  ser  recebidas  em  qualquer  estado  ou  apenas  em  estados
      terminais.
      </li>
      <li class="itemize">Recompensas  podem  ser  valores  que  o  modelo  tenta  maximizar  existentes  no
      sistema, como pontuação num jogo de tênis de mesa, ou valores simbólicos que
      funcionam como &#8220;dicas&#8221; de quão bem o agente está indo.
      </li>
      <li class="itemize">Os agentes podem ser passivos ou ativos quanto a aprendizagem. Os agentes passivos
      apenas observam o ambiente mudar e tenta entender o valor de estar em vários
      estados, enquanto o ativo age conforme aprende e pode usar o gerador do problema
      para explorar estados desconhecidos.</li></ul>
<!--l. 237--><p class="indent" >       <span 
class="ec-lmbx-12">??</span>) afirmam que o aprendizado por reforço depende bastante do conceito de estado e o
conceito de estado aplicado neste domínio é o estado de um MDP. Ainda segundo <span 
class="ec-lmbx-12">??</span>), MDP é
uma forma idealizada matematicamente de um problema de aprendizado por reforço para o
qual é possível realizar declarações teóricas precisas.
<!--l. 242--><p class="indent" >       Em geral, <span 
class="ec-lmbx-12">??</span>) definem MDP como uma quádrupla <span 
class="rm-lmr-12">(</span><span 
class="lmmi-12">S,K,R,T</span><span 
class="rm-lmr-12">)</span>, onde <span 
class="lmmi-12">S </span>é o conjunto
finito de possíveis estados, <span 
class="lmmi-12">K </span>é o conjunto finito de possíveis ações a se tomar, <span 
class="lmmi-12">R </span>é o conjunto
de recompensas imediatas para cada relação estado-ação e <span 
class="lmmi-12">T </span>é o conjunto de probabilidades de
transições para cada relação estado-ação.
<!--l. 247--><p class="indent" >       Existem diversos <span 
class="ec-lmri-12">designs </span>básicos para os agentes, como no aprendizado por reforço os
agentes devem receber recompensas de acordo com a utilidade da ação, <span 
class="ec-lmbx-12">??</span>) destacam dois
principais <span 
class="ec-lmri-12">designs</span>:
                                                                                       <img 
src="main79x.png" alt="PICT" >
<img 
src="main80x.png" alt="PICT" >
                                                                                       
      <ul class="itemize1">
      <li class="itemize">Onde o agente aprende a função de utilidade para os estados e usa isto para prever
      quais ações podem trazer um resultado melhor no futuro.
      </li>
      <li class="itemize">Quando o agente aprende uma função para a relação ação-estado e o retorno desta
      função define qual melhor ação tomar para cada estado.</li></ul>
<!--l. 259--><p class="indent" >       Este último <span 
class="ec-lmri-12">design </span>é a base do metódo a ser utilizado neste trabalho e é conhecido como
<span 
class="ec-lmri-12">Q-Learning</span>.
       <h4 class="subsectionHead"><span class="titlemark">2.2.1   </span> <a 
 id="x1-140002.2.1"></a>Q-Learning</h4>
<!--l. 264--><p class="noindent" >Segundo <span 
class="ec-lmbx-12">??</span>), <span 
class="ec-lmri-12">Q-Learning </span>&#8220;confere aos agentes a capacidade de aprender a agir otimamente em
um domínio Markoviano experimentando as consequências de suas ações sem a necessidade da
construção de mapas do domínio&#8221;.
<!--l. 268--><p class="indent" >       Como já explicado, estar em um ambiente Markoviano envolve um ambiente discreto
com número finito de estados e possíveis ações.
<!--l. 271--><p class="indent" >       <span 
class="ec-lmri-12">Q-Learning </span>utiliza uma ferramenta chamada de <span 
class="ec-lmri-12">Q-table</span>, de acordo com <span 
class="ec-lmbx-12">??</span>), esta
ferramenta consiste em uma tabela onde cada coluna corresponde a uma possível ação a ser
tomada e cada linha um possível estado a ser alcançado. Cada célula, então, corresponde ao
valor de recompensa máximo esperado para o par ação-estado, aqui chamado de <span 
class="lmmi-12">Q</span>. Um
exemplo de representação para esta tabela é demonstrada na Tabela <span 
class="ec-lmbx-12">??</span>. É possível perceber
que a tabela funciona como uma espécie de guia para escolher a melhor ação para o estado
atual.
       <div class="table">
                                                                                       <img 
src="main81x.png" alt="PICT" >
<img 
src="main82x.png" alt="PICT" >
                                                                                       
<!--l. 279--><p class="indent" >       <hr class="float"><div class="float" 
>
                                                                                       <img 
src="main83x.png" alt="PICT" >


